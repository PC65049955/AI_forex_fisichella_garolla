{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IsdEOhzsKU7",
        "outputId": "132b2466-2b69-4dd8-fdd0-0a4455795092"
      },
      "source": [
        "!pip install mplfinance\n",
        "!pip install Pillow\n",
        "!pip install -U tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mplfinance in /usr/local/lib/python3.7/dist-packages (0.12.7a17)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mplfinance) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mplfinance) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->mplfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mplfinance) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->mplfinance) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->mplfinance) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already up-to-date: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied, skipping upgrade: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhHG6RTSYkFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87959c54-37de-493d-8f85-ff66ad2a1831"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "!mkdir \"/content/sample_data/Models\" \n",
        "\n",
        "!mkdir \"/content/sample_data/Train\" \n",
        "!mkdir \"/content/sample_data/Train/sell\" \n",
        "!mkdir \"/content/sample_data/Train/buy\" \n",
        "!mkdir \"/content/sample_data/Train/stay\" \n",
        "\n",
        "!mkdir \"/content/sample_data/Val\" \n",
        "!mkdir \"/content/sample_data/Val/buy\" \n",
        "!mkdir \"/content/sample_data/Val/sell\" \n",
        "!mkdir \"/content/sample_data/Val/stay\" \n",
        "\n",
        "\n",
        "!mkdir \"/content/sample_data/Test\" \n",
        "!mkdir \"/content/sample_data/Test/buy\" \n",
        "!mkdir \"/content/sample_data/Test/sell\" \n",
        "!mkdir \"/content/sample_data/Test/stay\" \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/sample_data/Models’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Train’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Train/sell’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Train/buy’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Train/stay’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Val’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Val/buy’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Val/sell’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Val/stay’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Test’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Test/buy’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Test/sell’: File exists\n",
            "mkdir: cannot create directory ‘/content/sample_data/Test/stay’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRR4_Pu5Fxj2"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import mplfinance as fplt\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D,BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm \n",
        "import seaborn as sn\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8TF5EQOshR-",
        "outputId": "36aa7709-579a-4e80-eb9f-2b1a25dc9d58"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09IawRtiqHHK"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "SQLNg9wjIxMW",
        "outputId": "42c37c1c-bd78-444b-b196-256beeeee1f2"
      },
      "source": [
        "forex_df_data = pd.read_csv('/content/drive/My Drive/Forex/data/Test5_EURUSD_MarketMov/EURUSD/Seqs_2000_1_3_0.txt', sep=\";\")\n",
        "forex_df_labels = pd.read_csv('/content/drive/My Drive/Forex/data/Test5_EURUSD_MarketMov/EURUSD/Labels_2000_1_3_0.txt', sep=\";\")\n",
        "forex_df = pd.merge(forex_df_data, forex_df_labels, on='IDTime')\n",
        "forex_df.rename(columns={'ReadableTime': 'Date'}, inplace=True)\n",
        "forex_df['Date']= pd.to_datetime(forex_df['Date']) \n",
        "forex_df['Open'] = forex_df['Close']\n",
        "forex_df['Open'] = forex_df['Open'].shift(1)\n",
        "forex_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IDTime</th>\n",
              "      <th>Date</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>BarType</th>\n",
              "      <th>HAType</th>\n",
              "      <th>BBSignal</th>\n",
              "      <th>BBBreakoutSignal</th>\n",
              "      <th>StochMain</th>\n",
              "      <th>StochSignal</th>\n",
              "      <th>MarketMov</th>\n",
              "      <th>LabelStrategy</th>\n",
              "      <th>Class</th>\n",
              "      <th>Open</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>946857600</td>\n",
              "      <td>2000-01-03 00:00:00</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>1.0073</td>\n",
              "      <td>1.0175</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>84.57584</td>\n",
              "      <td>84.77898</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>sell</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>946872000</td>\n",
              "      <td>2000-01-03 04:00:00</td>\n",
              "      <td>1.0181</td>\n",
              "      <td>1.0154</td>\n",
              "      <td>1.0157</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>84.83516</td>\n",
              "      <td>83.13700</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>sell</td>\n",
              "      <td>1.0175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>946886400</td>\n",
              "      <td>2000-01-03 08:00:00</td>\n",
              "      <td>1.0167</td>\n",
              "      <td>1.0093</td>\n",
              "      <td>1.0115</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>78.31094</td>\n",
              "      <td>82.57398</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>buy</td>\n",
              "      <td>1.0157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>946900800</td>\n",
              "      <td>2000-01-03 12:00:00</td>\n",
              "      <td>1.0118</td>\n",
              "      <td>1.0054</td>\n",
              "      <td>1.0073</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62.99484</td>\n",
              "      <td>75.38031</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>buy</td>\n",
              "      <td>1.0115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>946915200</td>\n",
              "      <td>2000-01-03 16:00:00</td>\n",
              "      <td>1.0240</td>\n",
              "      <td>1.0070</td>\n",
              "      <td>1.0218</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>69.29825</td>\n",
              "      <td>70.20134</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>stay</td>\n",
              "      <td>1.0073</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      IDTime                Date    High  ...  LabelStrategy  Class    Open\n",
              "0  946857600 2000-01-03 00:00:00  1.0190  ...              0   sell     NaN\n",
              "1  946872000 2000-01-03 04:00:00  1.0181  ...              0   sell  1.0175\n",
              "2  946886400 2000-01-03 08:00:00  1.0167  ...              0    buy  1.0157\n",
              "3  946900800 2000-01-03 12:00:00  1.0118  ...              0    buy  1.0115\n",
              "4  946915200 2000-01-03 16:00:00  1.0240  ...              0   stay  1.0073\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc2WpqQLbhgE",
        "outputId": "33649f76-ba84-4d8a-9620-ae8e4ab1de1d"
      },
      "source": [
        "forex_ohlc = forex_df[[\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Class\"]].copy()\n",
        "dt_range = pd.date_range(start=\"2010.01.01\", end=\"2020.01.01\", freq='240min')\n",
        "forex_ohlc = forex_ohlc[forex_ohlc.Date.isin(dt_range)]\n",
        "forex_ohlc.reset_index(drop=True,inplace=True)\n",
        "forex_ohlc.head(5)\n",
        "total_size = forex_ohlc.shape[0]\n",
        "train_size = int(0.8*total_size)\n",
        "test_size = total_size - train_size\n",
        "min_val = forex_ohlc.Low.min()\n",
        "max_val = forex_ohlc.High.max()\n",
        "print(\"Train set size = {0} and Test set size = {1}\".format(train_size,test_size))\n",
        "train_df = forex_ohlc.iloc[:train_size,:]\n",
        "test_df = forex_ohlc.iloc[train_size:,:]\n",
        "\n",
        "train_df\n",
        "\n",
        "print(\"Training Dataset\\n\", train_df['Class'].value_counts(dropna=False))\n",
        "print(\"Test Dataset\\n\", test_df['Class'].value_counts(dropna=False))\n",
        "\n",
        "print(train_df.head(5))\n",
        "print(train_df.tail(5))\n",
        "print(test_df.head(5))\n",
        "print(test_df.tail(5))\n",
        "print(min_val, max_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set size = 12433 and Test set size = 3109\n",
            "Training Dataset\n",
            " stay    8871\n",
            "sell    1861\n",
            "buy     1701\n",
            "Name: Class, dtype: int64\n",
            "Test Dataset\n",
            " stay    2833\n",
            "sell     149\n",
            "buy      127\n",
            "Name: Class, dtype: int64\n",
            "                 Date     Open     High      Low    Close Class\n",
            "0 2010-01-04 00:00:00  1.43306  1.43336  1.42569  1.42939   buy\n",
            "1 2010-01-04 04:00:00  1.42939  1.43090  1.42718  1.42949   buy\n",
            "2 2010-01-04 08:00:00  1.42949  1.43994  1.42688  1.43778   buy\n",
            "3 2010-01-04 12:00:00  1.43778  1.44498  1.43768  1.44266  stay\n",
            "4 2010-01-04 16:00:00  1.44266  1.44543  1.44030  1.44120  stay\n",
            "                     Date     Open     High      Low    Close Class\n",
            "12428 2017-12-29 00:00:00  1.19426  1.19492  1.19361  1.19421  stay\n",
            "12429 2017-12-29 04:00:00  1.19421  1.19496  1.19376  1.19491   buy\n",
            "12430 2017-12-29 08:00:00  1.19491  1.19876  1.19460  1.19812  stay\n",
            "12431 2017-12-29 12:00:00  1.19812  1.19986  1.19734  1.19870  stay\n",
            "12432 2017-12-29 16:00:00  1.19870  1.20255  1.19855  1.20215  stay\n",
            "                     Date     Open     High      Low    Close Class\n",
            "12433 2017-12-29 20:00:00  1.20215  1.20239  1.19925  1.19957   buy\n",
            "12434 2018-01-02 00:00:00  1.19957  1.20234  1.20011  1.20155  stay\n",
            "12435 2018-01-02 04:00:00  1.20155  1.20246  1.20078  1.20246  stay\n",
            "12436 2018-01-02 08:00:00  1.20246  1.20706  1.20227  1.20704  stay\n",
            "12437 2018-01-02 12:00:00  1.20704  1.20813  1.20562  1.20574  stay\n",
            "                     Date     Open     High      Low    Close Class\n",
            "15537 2019-12-31 04:00:00  1.12126  1.12148  1.11988  1.12070  stay\n",
            "15538 2019-12-31 08:00:00  1.12070  1.12175  1.12030  1.12136  stay\n",
            "15539 2019-12-31 12:00:00  1.12136  1.12390  1.12086  1.12380  stay\n",
            "15540 2019-12-31 16:00:00  1.12380  1.12392  1.12162  1.12290  stay\n",
            "15541 2019-12-31 20:00:00  1.12290  1.12290  1.12226  1.12233  stay\n",
            "1.03249 1.49385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dISTwPC9STp-"
      },
      "source": [
        "train_df.set_index('Date', inplace=True)\n",
        "test_df.set_index('Date', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1YWm158SWF-"
      },
      "source": [
        "from random import random, seed\n",
        "seed(42)\n",
        "def generate_image(sample_df, save_location_template, fname, sample_stay=False):\n",
        "  class_name = sample_df.iloc[-1,4] # class of the last operation in the sequence\n",
        "  roll = 1\n",
        "  if sample_stay and class_name == 'stay':\n",
        "    roll = random()\n",
        "  if roll > 0.8: # only keep 20% of the stay samples because we have so many\n",
        "    fplt.plot(\n",
        "        sample_df,\n",
        "        type='candle',\n",
        "        style = \"yahoo\",\n",
        "      figratio=(1,1),axisoff=True,tight_layout = True,mav=(2,4,6),\n",
        "        savefig=dict(fname=save_location_template.format(class_name, fname),dpi=100),\n",
        "        #ylim=(min_val,max_val)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J93p-Q2yTj8K",
        "outputId": "402314dc-159a-4d45-db18-680d85c3b285"
      },
      "source": [
        "train_lookback = [20]\n",
        "TRAIN_FNAME_TEMPLATE = '/content/sample_data/Train/{}/yahoo_{}.png'\n",
        "for i in train_lookback:\n",
        "  for j in tqdm(range(train_df.shape[0] - i), desc=\"Creating Images progress\", bar_format='{l_bar}{bar}|'):\n",
        "      sample_df = train_df.iloc[j:j+i,: ]\n",
        "      generate_image(sample_df, TRAIN_FNAME_TEMPLATE, j, sample_stay=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Images progress: 100%|██████████|\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXN6KTQvBgw7",
        "outputId": "49ff7f18-a89f-48bd-c01b-92b9bc365185"
      },
      "source": [
        "VAL_FNAME_ROOT = '/content/sample_data/Val/'\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "_, val_fn = train_test_split(list(Path('/content/sample_data/Train').glob('*/*.png')), test_size=0.1, random_state=42)\n",
        "for p in tqdm(val_fn):\n",
        "  dst = Path(str(p).replace('/Train/','/Val/'))\n",
        "  p.replace(dst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 529/529 [00:00<00:00, 22948.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DYMsK0jIgNa",
        "outputId": "3bd0d26c-bf86-4ebb-b908-86e7ad1397a5"
      },
      "source": [
        "test_lookback = [20]\n",
        "TEST_FNAME_TEMPLATE = '/content/sample_data/Test/{}/yahoo_{}.png'\n",
        "for i in test_lookback:\n",
        "  for j in tqdm(range(test_df.shape[0] - i), desc=\"Creating Images progress\", bar_format='{l_bar}{bar}|'):\n",
        "      generate_image(test_df.iloc[j:j+i,: ], TEST_FNAME_TEMPLATE, j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Images progress: 100%|██████████|\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HahP5_4JXNcx"
      },
      "source": [
        "!ls /content/sample_data/Train/buy | wc -l\n",
        "!ls /content/sample_data/Train/sell | wc -l\n",
        "!ls /content/sample_data/Train/stay | wc -l\n",
        "!ls /content/sample_data/Val/buy | wc -l\n",
        "!ls /content/sample_data/Val/sell | wc -l\n",
        "!ls /content/sample_data/Val/stay | wc -l\n",
        "!ls /content/sample_data/Test/buy | wc -l\n",
        "!ls /content/sample_data/Test/sell | wc -l\n",
        "!ls /content/sample_data/Test/stay | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUnH4_xDSNV-"
      },
      "source": [
        "# Visualize dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIhLuhrgSPYn"
      },
      "source": [
        "import glob\n",
        "from random import shuffle\n",
        "matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
        "buy_fnames = glob.glob('/content/sample_data/Train/buy/*.png')\n",
        "sell_fnames = glob.glob('/content/sample_data/Train/sell/*.png')\n",
        "stay_fnames = glob.glob('/content/sample_data/Train/stay/*.png')\n",
        "shuffle(buy_fnames)\n",
        "shuffle(sell_fnames)\n",
        "shuffle(stay_fnames)\n",
        "\n",
        "ROWS = 4\n",
        "COLS = 4\n",
        "\n",
        "fig, axes = plt.subplots(ROWS, COLS)\n",
        "fig.suptitle('BUY')\n",
        "for r in range(ROWS):\n",
        "  for c in range(COLS):\n",
        "    im = plt.imread(buy_fnames[r*ROWS+c])\n",
        "    axes[r][c].set_axis_off()\n",
        "    axes[r][c].patch.set_edgecolor('black')  \n",
        "    axes[r][c].patch.set_linewidth('1')  \n",
        "    axes[r][c].imshow(im)\n",
        "    \n",
        "\n",
        "fig, axes = plt.subplots(ROWS, COLS)\n",
        "fig.suptitle('STAY')\n",
        "for r in range(ROWS):\n",
        "  for c in range(COLS):\n",
        "    im = plt.imread(stay_fnames[r*ROWS+c])\n",
        "    axes[r][c].set_axis_off()\n",
        "    axes[r][c].patch.set_edgecolor('black')  \n",
        "    axes[r][c].patch.set_linewidth('1')  \n",
        "    axes[r][c].imshow(im)\n",
        "\n",
        "fig, axes = plt.subplots(ROWS, COLS)\n",
        "fig.suptitle('SELL')\n",
        "for r in range(ROWS):\n",
        "  for c in range(COLS):\n",
        "    im = plt.imread(sell_fnames[r*ROWS+c])\n",
        "    axes[r][c].set_axis_off()\n",
        "    axes[r][c].patch.set_edgecolor('black')  \n",
        "    axes[r][c].patch.set_linewidth('1')  \n",
        "    axes[r][c].imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUCLN3AVqQCd"
      },
      "source": [
        "# Prepare network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qVp8_y0qTOa"
      },
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "  for units in hidden_units:\n",
        "    x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "  return x\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "  def __init__(self, patch_size):\n",
        "    super(Patches, self).__init__()\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  def call(self, images):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "      images=images,\n",
        "      sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "      strides=[1, self.patch_size, self.patch_size, 1],\n",
        "      rates=[1, 1, 1, 1],\n",
        "      padding=\"VALID\",\n",
        "    )\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "    return patches\n",
        "  \n",
        "class PatchEncoder(layers.Layer):\n",
        "  def __init__(self, num_patches, projection_dim):\n",
        "    super(PatchEncoder, self).__init__()\n",
        "    self.num_patches = num_patches\n",
        "    self.projection = layers.Dense(units=projection_dim)\n",
        "    self.position_embedding = layers.Embedding(\n",
        "      input_dim=num_patches, output_dim=projection_dim\n",
        "    )\n",
        "\n",
        "  def call(self, patch):\n",
        "    positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "    encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "    return encoded\n",
        "\n",
        "def create_vit_classifier(input_shape, data_preprocessing):\n",
        "  inputs = layers.Input(shape=input_shape)\n",
        "  # Augment data.\n",
        "  augmented = data_preprocessing(inputs)\n",
        "  # Create patches.\n",
        "  patches = Patches(patch_size)(augmented)\n",
        "  # Encode patches.\n",
        "  encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "  # Create multiple layers of the Transformer block.\n",
        "  for _ in range(transformer_layers):\n",
        "    # Layer normalization 1.\n",
        "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    # Create a multi-head attention layer.\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "    )(x1, x1)\n",
        "    # Skip connection 1.\n",
        "    x2 = layers.Add()([attention_output, encoded_patches])\n",
        "    # Layer normalization 2.\n",
        "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "    # MLP.\n",
        "    x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "    # Skip connection 2.\n",
        "    encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "  # Create a [batch_size, projection_dim] tensor.\n",
        "  representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "  representation = layers.Flatten()(representation)\n",
        "  representation = layers.Dropout(0.5)(representation)\n",
        "  # Add MLP.\n",
        "  features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "  # Classify outputs.\n",
        "  logits = layers.Dense(3)(features)\n",
        "  #logits = layers.Dense(1, activation='tanh')(features)\n",
        "  # Create the Keras model.\n",
        "  model = keras.Model(inputs=inputs, outputs=logits)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOrw1c0KqthY"
      },
      "source": [
        "## Test patch generation\n",
        "matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IMAGE_SIZE=224\n",
        "PATCH_SIZE=16\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = np.asarray(plt.imread('/content/sample_data/Train/buy/yahoo_10006.png'))[...,:3]\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
        ")\n",
        "patches = Patches(PATCH_SIZE)(resized_image)\n",
        "print(f\"Image size: {IMAGE_SIZE} X {IMAGE_SIZE}\")\n",
        "print(f\"Patch size: {PATCH_SIZE} X {PATCH_SIZE}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4), facecolor=(0,0,0))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (PATCH_SIZE, PATCH_SIZE, 3))\n",
        "    plt.imshow(patch_img.numpy())\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_GEHTd-vDwt"
      },
      "source": [
        "# Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDu8U3v3vGWq"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "image_size = 224  # We'll resize input images to this size\n",
        "patch_size = 16   # Size of the patches to be extracted from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7-iVjmovTHB"
      },
      "source": [
        "# Build input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt4E4AdaWDQG",
        "outputId": "eee35dfb-db4e-4323-c236-e2420d9ba9ee"
      },
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "LBL_MAP = {'buy':0,'sell':1,'stay':2}\n",
        "\n",
        "train_fnames = glob.glob('/content/sample_data/Train/*/*.png')\n",
        "val_fnames = glob.glob('/content/sample_data/Val/*/*.png')\n",
        "test_fnames = glob.glob('/content/sample_data/Test/*/*.png')\n",
        "\n",
        "x_train = np.asarray([np.asarray(Image.open(fn).convert('RGB').resize((image_size, image_size))) for fn in tqdm(train_fnames)])\n",
        "y_train = keras.utils.to_categorical(np.asarray([LBL_MAP[fn.split('/')[-2]] for fn in train_fnames]), num_classes=3)\n",
        "\n",
        "x_val   = np.asarray([np.asarray(Image.open(fn).convert('RGB').resize((image_size, image_size))) for fn in tqdm(val_fnames)])\n",
        "y_val   = keras.utils.to_categorical(np.asarray([LBL_MAP[fn.split('/')[-2]] for fn in val_fnames]), num_classes=3)\n",
        "\n",
        "x_test   = np.asarray([np.asarray(Image.open(fn).convert('RGB').resize((image_size, image_size))) for fn in tqdm(test_fnames)])\n",
        "y_test   = keras.utils.to_categorical(np.asarray([LBL_MAP[fn.split('/')[-2]] for fn in test_fnames]), num_classes=3)\n",
        "\n",
        "\n",
        "data_preprocessing = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.Normalization()\n",
        "    ],\n",
        "    name=\"data_preprocessing\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_preprocessing.layers[0].adapt(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4755/4755 [00:48<00:00, 97.14it/s] \n",
            "100%|██████████| 529/529 [00:05<00:00, 101.99it/s]\n",
            "100%|██████████| 3089/3089 [00:32<00:00, 96.24it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOyc34V50AWK"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtOgLgj0z_NU"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def run_experiment(model, x_train, y_train, x_val, y_val, x_test, y_test, model_name):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = f\"/content/drive/MyDrive/Models/{model_name}/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=(x_val,y_val),\n",
        "        validation_batch_size=batch_size,\n",
        "        callbacks=[checkpoint_callback],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = model.evaluate(x_train, y_train)\n",
        "    print('\\nPerformance of best model:')\n",
        "    print(f\"> Train accuracy: {accuracy}\")\n",
        "    _, accuracy = model.evaluate(x_val, y_val)\n",
        "    print(f\"> Val accuracy: {accuracy}\")\n",
        "    _, accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"> Test accuracy: {accuracy}\")\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "    cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "    print('Confusion Matrix:')\n",
        "    print(cm)\n",
        "\n",
        "    return history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "9vlE7Xx80UDx",
        "outputId": "3589acfc-37a8-419f-a9ca-8768227cfa51"
      },
      "source": [
        "vit_classifier = create_vit_classifier((image_size, image_size, 3),data_preprocessing)\n",
        "history = run_experiment(vit_classifier, x_train, y_train, x_val, y_val, x_test, y_test, 'ViT_224')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "149/149 [==============================] - 148s 983ms/step - loss: 0.4128 - accuracy: 0.8801\n",
            "\n",
            "Performance of best model:\n",
            "> Train accuracy: 0.8801261782646179\n",
            "17/17 [==============================] - 17s 975ms/step - loss: 0.4040 - accuracy: 0.8790\n",
            "> Val accuracy: 0.8790169954299927\n",
            "97/97 [==============================] - 95s 979ms/step - loss: 1.1233 - accuracy: 0.5040\n",
            "> Test accuracy: 0.504046618938446\n",
            "Confusion Matrix:\n",
            "[[  36   34   55]\n",
            " [  34   47   68]\n",
            " [ 556  785 1474]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-511f5dd15e2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvit_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vit_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ViT_224'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#vit_classifier.save(f\"/content/drive/MyDrive/Models/ViT_224/saved_model\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#drive.flush_and_unmount()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#drive.mount('/content/drive', force_remount=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-5e6cfef46499>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model, x_train, y_train, x_val, y_val, x_test, y_test, model_name)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sik8V991wqr"
      },
      "source": [
        "from collections import Counter\n",
        "y_pred = vit_classifier(x_val)\n",
        "Counter(y_pred.numpy().argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3MWIc205J3V"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = vit_classifier(x_test)\n",
        "confusion_matrix(y_test.argmax(axis=1), y_pred.numpy().argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lldwAB99Sx2k"
      },
      "source": [
        "# ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4C9wCtbC9ej"
      },
      "source": [
        "x_train_resnet = tf.keras.applications.resnet.preprocess_input(x_train)\n",
        "x_val_resnet = tf.keras.applications.resnet.preprocess_input(x_val)\n",
        "x_test_resnet = tf.keras.applications.resnet.preprocess_input(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD6UcND8IiqN"
      },
      "source": [
        "# Delete the non-resnet x data to save memory\n",
        "del x_train\n",
        "del x_val\n",
        "del x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfccdNeES0jn"
      },
      "source": [
        "resnet_classifier = tf.keras.models.Sequential([\n",
        "  tf.keras.applications.ResNet50(include_top=False, input_shape=(224,224,3)),\n",
        "  layers.GlobalAveragePooling2D(name='avg_pool'),\n",
        "  layers.Dense(3, name='logits')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfwdsXauTaUd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "c6bde6c1-84dd-4c0a-be49-f5053cfe9d96"
      },
      "source": [
        "history = run_experiment(resnet_classifier, x_train_resnet, y_train, x_val_resnet, y_val, x_test_resnet, y_test, 'ResNet50')\n",
        "resnet_classifier.save(f\"/content/drive/MyDrive/Models/ResNet50/saved_model\")\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[   0   62   63]\n",
            " [   2   64   83]\n",
            " [  26  978 1811]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
